---
title: "Detecting Hate Speech In Multimodal Memes Using Vision-Language Models"
collection: publications
permalink: /publication/2021-04-msc-thesis
excerpt: 'multimodal learning, vision-and-language, meme classification, master thesis'
date: 2021-04-21
venue: 'https://pub.uni-bielefeld.de/'
paperurl: 
citation:
---

<b>Abstract---</b>
Memes on the Internet are often harmless and sometimes amusing. The apparently innocent meme, though, becomes a multimodal form of hate speech when certain kinds of pictures, text, or variations of both are used â€“ a *hateful meme*. The [Hateful Memes Challenge](https://hatefulmemeschallenge.com/) is a one-of-a-kind competition that focuses on detecting hate speech in multimodal memes and proposes a new data collection with 10,000+ new examples of multimodal content. We use VisualBERT, which is also known as "BERT for vision and language," and Ensemble Learning to boost the performance. In the Hateful Memes Challenge, our solution received an AUROC of 0.811 and an accuracy of 0.765 on the challenge test set, placing us **third out of 3,173 participants**. The code is available on [GitHub](https://github.com/rizavelioglu/hateful_memes-hate_detectron).

[Download the thesis here!](https://pub.uni-bielefeld.de/record/2989295)


Cited as:

```
@mastersthesis{velioglu2021detecting,
  title   = "Detecting Hate Speech In Multimodal Memes Using Vision-Language Models",
  author  = "Velioglu, Riza",
  school  = "Bielefeld University",
  year    = "2021",
  url     = "https://pub.uni-bielefeld.de/record/2989295"
}
```