---
title: "A comparative study of neural machine translation models for Turkish language"
collection: publications
permalink: /publication/2022-02-nmt
excerpt: 'Neural machine translation, sequence to sequence, transformer'
date: 2022-02-02
venue: 'Journal of Intelligent & Fuzzy Systems'
paperurl:
citation:
---

<b>Abstract---</b> Machine translation (MT) is an important challenge in the fields of Computational Linguistics. In 
this study, we conducted neural machine translation (NMT) experiments on two different architectures. First, Sequence to
Sequence (Seq2Seq) architecture along with a variation that utilizes attention mechanism is performed on translation 
task. Second, an architecture that is fully based on the self-attention mechanism, namely Transformer, is employed to 
perform a comprehensive comparison. Besides, the contribution of employing Byte Pair Encoding (BPE) and Gumbel Softmax 
distributions are examined for both architectures. The experiments are conducted on two different datasets: TED Talks 
that is one of the popular benchmark datasets for NMT especially among morphologically rich languages like Turkish and 
WMT18 News dataset that is provided by The Third Conference on Machine Translation (WMT) for shared tasks on various 
aspects of machine translation. The evaluation of Turkish-to-English translationsâ€™ results demonstrate that the 
Transformer model with combination of BPE and Gumbel Softmax achieved 22.4 BLEU score on TED Talks and 38.7 BLUE score 
on WMT18 News dataset. The empirical results support that using Gumbel Softmax distribution improves the quality of 
translations for both architectures.

[Download paper here](https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs211453)

Cited as:

```
@article{ozdemir2022comparative,
  title={A comparative study of neural machine translation models for Turkish language},
  author={Ozdemir, Ozgur and Akin, Emre Salih and Velioglu, Riza and Dalyan, Tugba},
  journal={Journal of Intelligent \& Fuzzy Systems},
  number={Preprint},
  pages={1--11},
  publisher={IOS Press}
}
```
